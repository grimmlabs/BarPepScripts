{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53d086b-a870-4063-add7-845d3bb1b0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO, Align, Seq\n",
    "import time\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import argparse\n",
    "from argparse import RawTextHelpFormatter\n",
    "import os\n",
    "import pdb\n",
    "import pandas as pd\n",
    "from contextlib import nullcontext\n",
    "from gzip import open as gzopen\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297ff776-3031-40e6-a3bf-7996e769cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_barcodes(read_name, read_sequence, barcodes, aligner, threshhold):\n",
    "    qualities = {}\n",
    "    qualities[read_name] = []\n",
    "    for BC_name, BC_fwd in barcodes.items():\n",
    "        alignments = aligner.align(read_sequence, BC_fwd)\n",
    "        for alignment in sorted(alignments):\n",
    "            if alignment.score >= threshhold:\n",
    "                qualities[read_name].append([alignment.score, alignment, BC_name, read_sequence])\n",
    "    return(qualities)\n",
    "\n",
    "def filter_alignments_score(x):\n",
    "    \"\"\"\n",
    "    Keeps only the best alignment.\n",
    "    Reads with multiple best alignments are put into the multibarcoded category (weird flag).\n",
    "    Reads with no alignments into the no barcodes category.\n",
    "    \"\"\"\n",
    "    weird_reads = []\n",
    "    unbarcoded_reads = []\n",
    "    new_dict = {}\n",
    "    for read_name, aln_list in x.items():\n",
    "        weird_flag = False\n",
    "        aln_list = np.array(aln_list, dtype=object)\n",
    "        if len(aln_list) == 0: # if there are no alignments\n",
    "            unbarcoded_reads.append(read_name)\n",
    "            continue\n",
    "        score_list = np.array([x[0] for x in aln_list])\n",
    "        max_score = int(np.max(score_list))\n",
    "        # check if the maximum score appears more than once\n",
    "        max_score_count = np.sum(score_list == max_score)\n",
    "        if max_score_count > 1:\n",
    "            # make a list of all the barcodes with the maximum score\n",
    "            max_score_aln_list = [x[2] for x in aln_list[score_list == max_score]]\n",
    "            # when the aligned, top-score barcodes are the same then do nothing\n",
    "            # the first alignment is then chosen\n",
    "            if len(np.unique(max_score_aln_list)) != 1:\n",
    "                # the read has at least 2 different barcodes aligned with the same quality\n",
    "                # set the weird flag\n",
    "                weird_flag = True\n",
    "        if weird_flag:\n",
    "            weird_reads.append(read_name)\n",
    "            continue\n",
    "        new_dict[read_name] = [aln_list[np.argmax(score_list)]]\n",
    "    return(new_dict, unbarcoded_reads, weird_reads)\n",
    "\n",
    "def prepare_arguments(x, barcodes, aligner, threshold):\n",
    "    # prepare arguments for mp\n",
    "    async_readnames_list = list(x.keys())\n",
    "    async_sequences_list = [x[j] for j in async_readnames_list]\n",
    "    async_barcode_list = [barcodes for j in range(len(x))]\n",
    "    async_aligner_list = [aligner for j in range(len(x))]\n",
    "    async_thresh_list = [threshold for i in range(len(x))]\n",
    "\n",
    "    starmap_args = []\n",
    "    for i in range(len(x)):\n",
    "        starmap_args.append((\n",
    "            async_readnames_list[i],\n",
    "            async_sequences_list[i],\n",
    "            async_barcode_list[i],\n",
    "            async_aligner_list[i],\n",
    "            async_thresh_list[i]))\n",
    "    return(starmap_args)\n",
    "\n",
    "def combine_dict(d1, d2):\n",
    "    return {\n",
    "        k: tuple(d[k] for d in (d1, d2) if k in d)\n",
    "        for k in list(set(d1.keys()) | set(d2.keys()))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62effe32-a620-4dda-a012-f10298980904",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"variants\": \"/run/user/1000/gvfs/smb-share:server=home.isilon.bioquant.uni-heidelberg.de,share=nwg-grimm/scripts_NGSanalysis/ctest_data/BC_variants.txt\",\n",
    "    \"input\": \"/run/user/1000/gvfs/smb-share:server=home.isilon.bioquant.uni-heidelberg.de,share=nwg-grimm/scripts_NGSanalysis/ctest_data/fastqs/test/\",\n",
    "    \"open_gap\": -1,\n",
    "    \"extend_gap\": -10,\n",
    "    \"mismatch\": -1,\n",
    "    \"match\": 1,\n",
    "    \"threshold\": 4,\n",
    "    \"out_directory\": \"example_out_HAC\",\n",
    "    \"threads\": 8,\n",
    "    \"barcode_length\": None,\n",
    "    \"left\": \"GGCTGG\",\n",
    "    \"right\": \"TGGGCC\",\n",
    "    \"left_thresh\": None,\n",
    "    \"right_thresh\": None,\n",
    "    \"verbose\": True\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d09e9c-5801-47b5-b3fc-9cc3c314230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_reverse = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe853f9-4026-4a4f-a32f-9954cec58f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "## aligner parameters\n",
    "aligner = Align.PairwiseAligner()\n",
    "aligner.mode = 'local'\n",
    "aligner.open_gap_score = int(args[\"open_gap\"])\n",
    "aligner.extend_gap_score = int(args[\"extend_gap\"])\n",
    "aligner.mismatch_score = int(args[\"mismatch\"])\n",
    "aligner.match_score = int(args[\"match\"])\n",
    "## barcodes threshold\n",
    "barcode_threshold = int(args[\"threshold\"])\n",
    "\n",
    "## peptide threshold\n",
    "left_threshold = args[\"left_thresh\"]\n",
    "if left_threshold == None:\n",
    "    left_threshold = len(args[\"left\"])-1\n",
    "right_threshold = args[\"right_thresh\"]\n",
    "if right_threshold == None:\n",
    "    right_threshold = len(args[\"right\"])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "496f540b-f3f1-48a7-bab2-e3f852148d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(left_threshold)\n",
    "print(right_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e22d3bf6-aa71-412c-91b5-af79d5d0feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcodes_FWD = pd.read_csv(args[\"variants\"], names = [\"sequence\", \"name\"], sep=\"\\t\").set_index(\"name\").to_dict()[\"sequence\"]\n",
    "if use_reverse:\n",
    "    barcodes_RC = {name+\"RC\":str(Seq.Seq(bc).reverse_complement()) for name, bc in barcodes_FWD.items()}\n",
    "    barcodes = barcodes_RC\n",
    "else:\n",
    "    barcodes = barcodes_FWD\n",
    "\n",
    "if args[\"barcode_length\"] == None:\n",
    "    barcode_length = len(list(barcodes.values())[-1])\n",
    "else:\n",
    "    barcode_length = int(args[\"barcode_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f300bbe4-dacf-46dd-aae3-ca4bad8b5a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AAV2': 'GCTCTGGATGTAGTA',\n",
       " 'S0112coAAV2': 'TATCAAGCTAACGTT',\n",
       " 'S0312coAAV2': 'GTCAACATCGTTACA',\n",
       " 'S0412coAAV2': 'GGGCCCTAGCGCGTG',\n",
       " 'S0512coAAV2': 'GATAGGCTGGTCCAA',\n",
       " 'S0912coAAV2': 'TATTTGTGTCGTTCC',\n",
       " 'S1112coAAV2': 'AGTTAGGGCGCTGCG',\n",
       " 'S1312coAAV2': 'GCCCTTCAGTCAGCT',\n",
       " 'S1612coAAV2': 'CGGTCGCGTGACGTG',\n",
       " 'S1812coAAV2': 'GCCGGAGTCCCGGTA',\n",
       " 'AAV9': 'TGTTGGAAGGTATCA',\n",
       " 'S0112coAAV9': 'GACTTGGTTGTGACG',\n",
       " 'S0312coAAV9': 'TTGTTGTATGAGCAG',\n",
       " 'S0412coAAV9': 'CTACCTATTTACTCT',\n",
       " 'S0512coAAV9': 'ACCGGGCGTTGAGGC',\n",
       " 'S0912coAAV9': 'TGGTTTACAAATTAT',\n",
       " 'S1112coAAV9': 'GTTGTGCCCTGAGTG',\n",
       " 'S1312coAAV9': 'ACCGTATCTCTCCGG',\n",
       " 'S1612coAAV9': 'TTGGAACGTGGGCTT',\n",
       " 'S1812coAAV9': 'AGATTCAAAGCTGCG',\n",
       " 'AAV5': 'AGCCTAATCTTTGAC',\n",
       " 'AAV8': 'AAGCACTAAAGAACA',\n",
       " 'AAV_DJ': 'GGTATGGCCTGCCGC'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8748b1df-baf5-497f-a95a-8c052af470fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## out directory\n",
    "out_directory = args[\"out_directory\"]\n",
    "if out_directory[-1] != \"/\":\n",
    "    out_directory += \"/\"\n",
    "if not os.path.exists(out_directory):\n",
    "    os.mkdir(out_directory)\n",
    "## multiprocessing\n",
    "n_workers = args[\"threads\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47207ffe-fb3c-47dc-921e-6bd9fca707e1",
   "metadata": {},
   "source": [
    "# BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ca44e-673e-4ca2-bc54-2bf8225fa8bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_start_time = time.time()\n",
    "\n",
    "for file in os.listdir(args[\"input\"]):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    file_path = os.path.join(args[\"input\"], file)\n",
    "    # conditional with clause\n",
    "    with gzopen(file_path, \"rt\") if file_path.endswith(\".gz\") else nullcontext() as file_path:\n",
    "\n",
    "        if file_path == None:\n",
    "            file_path = os.path.join(args[\"input\"], file)\n",
    "    \n",
    "        ## fastq input\n",
    "        fastq = {entry.description: str(entry.seq) for entry in SeqIO.parse(file_path, \"fastq\")}\n",
    "\n",
    "        \n",
    "        ## info\n",
    "        read_lengths = [len(x) for x in fastq.values()]\n",
    "        mean_read_length = sum(read_lengths) / len(read_lengths)\n",
    "        n_total = len(fastq)\n",
    "\n",
    "\n",
    "        global fastq_input\n",
    "        global results\n",
    "        with open(\"tmp\", \"wb\") as tmp:\n",
    "            for item in prepare_arguments(fastq, barcodes, aligner, barcode_threshold):\n",
    "                pickle.dump(item, tmp)\n",
    "                tmp.write(os.linesep.encode(\"UTF-8\"))\n",
    "        del fastq\n",
    "\n",
    "        # Main computation\n",
    "        ## parallel workers\n",
    "        p = mp.Pool(n_workers)\n",
    "        start_time = time.time()\n",
    "        ## do the calculation\n",
    "        with open(\"tmp\", \"rb\") as tmp:\n",
    "            results = p.starmap(\n",
    "                align_barcodes,\n",
    "                (pickle.loads(line) for line in tmp)\n",
    "            )\n",
    "        \n",
    "            ## combine the results\n",
    "            qualities = {}\n",
    "            for r in results:\n",
    "                qualities.update(r)\n",
    "    \n",
    "            del results\n",
    "        \n",
    "            ## filter the alignments of every read, to only retain the best one\n",
    "            filt_qualities, unbarcoded_reads, weird_reads = filter_alignments_score(qualities)\n",
    "        \n",
    "            # output\n",
    "            ## make output dictionary\n",
    "            out_dictionary = {}\n",
    "            for barcode_name in list(barcodes.keys()):\n",
    "                out_dictionary[barcode_name] = []\n",
    "        \n",
    "            ## populate output dict\n",
    "            for read_name, aln_list in filt_qualities.items():\n",
    "                out_dictionary[aln_list[0][2]].append([read_name, aln_list[0][3]])\n",
    "\n",
    "\n",
    "\n",
    "    df_out = pd.DataFrame({key:[barcodes[key], len(x)] for key, x in out_dictionary.items()}, index = [\"sequence\", \"count\"]).transpose()\n",
    "    # meta info\n",
    "    n_recovered = df_out[\"count\"].sum()\n",
    "    \n",
    "\n",
    "    time_taken = round(time.time() - start_time, 4)\n",
    "    \n",
    "    # Output\n",
    "    df_out.to_csv(os.path.join(args[\"out_directory\"], file).split(\".\")[0]+\".BC.csv\")\n",
    "\n",
    "    \n",
    "    # write log file\n",
    "    log_output_file = os.path.join(args[\"out_directory\"], file).split(\".\")[0]+\".BC.log.txt\"\n",
    "    f=open(log_output_file,'w')\n",
    "    f.write(\"File: \" + file + \"s\\n\")\n",
    "    f.write(\"Time taken: \" + str(time_taken) +  \"\\n\")\n",
    "    f.write(\"Total number of reads: \" + str(n_total) + \"\\n\")\n",
    "    f.write(\"Mean sequence length: \" + str(mean_read_length) + \" bp\\n\")\n",
    "    f.write(\"Reads recovered: \" + str(n_recovered) + \" (\" + str(round(n_recovered/n_total*100, 2)) + \"%)\\n\")\n",
    "    #f.write(\"\\nMean sequence quality: \"+str(round(mean_quality, 2)))\n",
    "    f.close()\n",
    "\n",
    "    # terminal output\n",
    "    if args[\"verbose\"]:\n",
    "        print(\"\\nFile: \" + file)\n",
    "        print(\"Time taken: \" + str(time_taken) + \"s\")\n",
    "        print(\"Total number of reads: \" + str(n_total))\n",
    "        print(\"Mean sequence length: \" + str(mean_read_length) + \" bp\")\n",
    "        print(\"Reads recovered: \" + str(n_recovered) + \" (\" + str(round(n_recovered/n_total*100, 2)) + \"%)\\n\")\n",
    "\n",
    "    del df_out, filt_qualities, unbarcoded_reads, weird_reads, qualities, out_dictionary\n",
    "\n",
    "\n",
    "total_time_taken = round(time.time() - total_start_time, 4)\n",
    "print(\"Total time taken: \" + str(total_time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012d186-f1da-4fdd-9e06-cb720389f16f",
   "metadata": {},
   "source": [
    "# PV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "1f3e321e-2e7f-4aa7-9b6b-33d3cccd572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare flanking regions\n",
    "if use_reverse:\n",
    "    Ldic = {\"left\": str(Seq.Seq(args[\"right\"]).reverse_complement())}\n",
    "    Cleft_threshold = right_threshold\n",
    "    Rdic = {\"right\": str(Seq.Seq(args[\"left\"]).reverse_complement())}\n",
    "    Cright_threshold = left_threshold\n",
    "else:\n",
    "    Ldic = {\"left\": args[\"left\"]}\n",
    "    Cleft_threshold = left_threshold\n",
    "    Rdic = {\"right\": args[\"right\"]}\n",
    "    Cright_threshold = right_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "e2abe408-0164-4e68-8733-0669c697008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File: m1_kidney.fastq.gz\n",
      "Total number of reads: 47839\n",
      "Mean sequence length: 122.0 bp\n",
      "Reads recovered: 45078 (94.23%)\n",
      "Mean peptide length: 21.028 bp\n",
      "\n",
      "File: m1_input.fastq.gz\n",
      "Total number of reads: 47839\n",
      "Mean sequence length: 122.0 bp\n",
      "Reads recovered: 45033 (94.13%)\n",
      "Mean peptide length: 21.039 bp\n",
      "\n",
      "File: m1_liver.fastq.gz\n",
      "Total number of reads: 47839\n",
      "Mean sequence length: 122.0 bp\n",
      "Reads recovered: 45175 (94.43%)\n",
      "Mean peptide length: 21.039 bp\n",
      "\n",
      "File: m1_lung.fastq.gz\n",
      "Total number of reads: 47839\n",
      "Mean sequence length: 122.0 bp\n",
      "Reads recovered: 45117 (94.31%)\n",
      "Mean peptide length: 21.04 bp\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(args[\"input\"]):\n",
    "    file_path = os.path.join(args[\"input\"], file)\n",
    "    # conditional with clause\n",
    "    with gzopen(file_path, \"rt\") if file_path.endswith(\".gz\") else nullcontext() as file_path:\n",
    "\n",
    "        if file_path == None:\n",
    "            file_path = os.path.join(args[\"input\"], file)\n",
    "\n",
    "        ## fastq input\n",
    "        global fastq_input\n",
    "        fastq = {entry.description: str(entry.seq) for entry in SeqIO.parse(file_path, \"fastq\")}\n",
    "        fastq_input = prepare_arguments(fastq, Ldic, aligner, Cleft_threshold)\n",
    "\n",
    "        ## info\n",
    "        read_lengths = [len(x) for x in fastq.values()]\n",
    "        mean_read_length = sum(read_lengths) / len(read_lengths)\n",
    "\n",
    "\n",
    "        # Main computation\n",
    "        ## parallel workers\n",
    "        p = mp.Pool(n_workers)\n",
    "        start_time = time.time()\n",
    "        ## do the calculation\n",
    "        Lresults = p.starmap(\n",
    "            align_barcodes,\n",
    "            fastq_input\n",
    "        )\n",
    "\n",
    "        Lqualities = {}\n",
    "        for r in Lresults:\n",
    "            Lqualities.update(r)\n",
    "        Lfilt_qualities, Lunbarcoded_reads, Lweird_reads = filter_alignments_score(Lqualities)\n",
    "\n",
    "        Rresults = p.starmap(\n",
    "            align_barcodes,\n",
    "            fastq_input\n",
    "        )\n",
    "\n",
    "        Rqualities = {}\n",
    "        for r in Rresults:\n",
    "            Rqualities.update(r)\n",
    "        Rfilt_qualities, Runbarcoded_reads, Rweird_reads = filter_alignments_score(Rqualities)\n",
    "\n",
    "        # if a read has both left and right flanks found combine them together \n",
    "        # the new dictionary contains the readname as key and left and right alignment in a tuple\n",
    "        reads_with_flank = combine_dict(Lfilt_qualities, Rfilt_qualities)\n",
    "        reads_with_peptide = {}\n",
    "        n_reads_only_1_flanking = 0\n",
    "        for readname in reads_with_flank:\n",
    "            # check if both flanking sequences were found\n",
    "            if len(reads_with_flank[readname]) == 2:\n",
    "                # get coordinates of the insert between the flanking sequences\n",
    "                start_peptide = reads_with_flank[readname][0][0][1].path[1][0]\n",
    "                end_peptide = reads_with_flank[readname][1][0][1].path[0][0]\n",
    "                # extract peptide and record\n",
    "                peptide = fastq[readname][start_peptide:end_peptide]\n",
    "                if len(peptide) > 0:\n",
    "                    reads_with_peptide[readname] = fastq[readname][start_peptide:end_peptide]\n",
    "                else:\n",
    "                    n_reads_only_1_flanking += 1\n",
    "            else:\n",
    "                n_reads_only_1_flanking += 1\n",
    "\n",
    "\n",
    "        n_recovered = len(reads_with_peptide)\n",
    "        n_total = len(fastq)\n",
    "        n_correct_length = sum(len(pep) == 21 for pep in reads_with_peptide.values())\n",
    "\n",
    "        pep_length = [len(x) for x in reads_with_peptide.values()]\n",
    "        mean_pep_length = round(sum(pep_length) / len(pep_length), 3)\n",
    "\n",
    "    \n",
    "    # add up variants\n",
    "    df_out = pd.DataFrame(pd.Series(reads_with_peptide.values()).value_counts())\n",
    "\n",
    "    # output\n",
    "    df_out.to_csv(os.path.join(args[\"out_directory\"], file).split(\".\")[0] + \".PV.csv\")\n",
    "\n",
    "    # write log file\n",
    "    log_output_file = os.path.join(args[\"out_directory\"], file).split(\".\")[0]+\".PV.log.txt\"\n",
    "    f=open(log_output_file,'w')\n",
    "    f.write(\"File: \" + file + \"\\n\")\n",
    "    f.write(\"Total number of reads: \" + str(n_total) + \"\\n\")\n",
    "    f.write(\"Mean sequence length: \" + str(mean_read_length) + \" bp\\n\")\n",
    "    f.write(\"Reads recovered: \" + str(n_recovered) + \" (\" + str(round(n_recovered/n_total*100, 2)) + \"%)\\n\")\n",
    "    f.write(\"Mean peptide length: \" + str(mean_pep_length) + \" bp\\n\")\n",
    "    #f.write(\"\\nMean sequence quality: \"+str(round(mean_quality, 2)))\n",
    "    f.close()\n",
    "\n",
    "    # terminal output\n",
    "    if args[\"verbose\"]:\n",
    "        print(\"\\nFile: \" + file)\n",
    "        print(\"Total number of reads: \" + str(n_total))\n",
    "        print(\"Mean sequence length: \" + str(mean_read_length) + \" bp\")\n",
    "        print(\"Reads recovered: \" + str(n_recovered) + \" (\"+str(round(n_recovered/n_total*100, 2))+\"%)\")\n",
    "        print(\"Mean peptide length: \" + str(mean_pep_length) + \" bp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0f66ea-fcb7-45c0-b70e-11b739ffd4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
